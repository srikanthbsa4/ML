{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcaZb2jxXE1akN9bWVOjJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikanthbsa4/ML/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPg5cHyrX0IT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> SKlearn\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ik9IfsgQX6FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_iris\n",
        "data=load_iris()\n",
        "print(data)\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)\n",
        "model=LinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "mse=mean_squared_error(y_pred,y_test)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5, color='blue')\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='green', linewidth=2)\n",
        "plt.title('Predicted vs Actual Prices')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "# plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yQIJrl87rICq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Finds"
      ],
      "metadata": {
        "id": "X8bx7l__rO-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=np.array([['sunny','warm','normal','strong','warm ','same'],\n",
        "               ['sunny','warm','high','strong','warm ','same'],\n",
        "['rainy','cold','normal','strong','warm ','change'],\n",
        "['sunny','warm','high','strong','cool','change']])\n",
        "target=[\"yes\",\"yes\",\"no\",\"yes\"]\n",
        "def finds(data,target):\n",
        "   specific_hyp=data[0].copy()\n",
        "   for i, instance in enumerate(data):\n",
        "      if target[i]==\"Yes\":\n",
        "          for j in range(len(specific_hyp)):\n",
        "              if specific_hyp[j]!=instance[j]:\n",
        "                  specific_hyp[j]='?'\n",
        "\n",
        "   return specific_hyp\n",
        "\n",
        "\n",
        "final=finds(data,target)\n",
        "\n",
        "\n",
        "feat=data1[['Color','Type','Origin','Weight']].values\n",
        "tar=data1['Preference'].values"
      ],
      "metadata": {
        "id": "bSw_txMxrr0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Candidate"
      ],
      "metadata": {
        "id": "ggBJLF82r-Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Function to learn from the DataFrame\n",
        "def learn(concepts,target):\n",
        "    # Extracting concepts (features) and target (labels) from the DataFrame\n",
        "\n",
        "\n",
        "    # Initialization\n",
        "    specific_h = concepts[0].copy()\n",
        "    print(\"Initialization of specific_h:\\n\", specific_h)\n",
        "    general_h = [[\"?\" for i in range(len(specific_h))] for i in range(len(specific_h))]\n",
        "    print(\"Initialization of general_h:\\n\", general_h)\n",
        "\n",
        "    # Iterate over each example in the dataset\n",
        "    for i, h in enumerate(concepts):\n",
        "        if target[i] == \"yes\":\n",
        "            print(\"If instance is Positive:\")\n",
        "            for x in range(len(specific_h)):\n",
        "                if h[x] != specific_h[x]:\n",
        "                    specific_h[x] = \"?\"\n",
        "        elif target[i] == \"no\":\n",
        "            print(\"If instance is Negative:\")\n",
        "            for x in range(len(specific_h)):\n",
        "                if h[x] != specific_h[x]:\n",
        "                    general_h[x][x] = specific_h[x]\n",
        "                else:\n",
        "                    general_h[x][x] = \"?\"\n",
        "\n",
        "        # Print intermediate results\n",
        "        print(f\"Step {i+1}:\")\n",
        "        print(\"Specific_h:\", specific_h)\n",
        "        print(\"General_h:\", general_h)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # Remove redundant general hypotheses\n",
        "    general_h = [h for h in general_h if h != [\"?\" for _ in range(len(specific_h))]]\n",
        "\n",
        "    return specific_h, general_h\n",
        "\n",
        "\n",
        "# Example usage with a DataFrame\n",
        "# Create a sample DataFrame\n",
        "data=np.array([['sunny','warm','normal','strong','warm ','same'],\n",
        "               ['sunny','warm','high','strong','warm ','same'],\n",
        "['rainy','cold','normal','strong','warm ','change'],\n",
        "['sunny','warm','high','strong','cool','change']])\n",
        "target=[\"yes\",\"yes\",\"no\",\"yes\"]\n",
        "\n",
        "# Run the learning function\n",
        "s_final, g_final = learn(data,target)\n",
        "\n",
        "# Print final hypotheses\n",
        "print(\"Final Specific_h:\", s_final, sep=\"\\n\")\n",
        "print(\"Final General_h:\", g_final, sep=\"\\n\")\n"
      ],
      "metadata": {
        "id": "ecZ_Jm95sASi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ID3"
      ],
      "metadata": {
        "id": "A9ife0RZsDZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with ID3-like settings\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Visualize the tree\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"diabetes.csv\")  # Replace with your file path\n",
        "X = data.iloc[:,0:8].values # Adjust feature names as needed\n",
        "y = data.iloc[:,-1].values # Target: orange or apple\n",
        "\n",
        "# Convert categorical features to numeric (if needed)\n",
        "# X = pd.get_dummies(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier(criterion = \"entropy\", random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(25,10))\n",
        "plot_tree(clf, filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7UJc9YdGsGbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB"
      ],
      "metadata": {
        "id": "wX-X7_sOsZTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NAIVE BAYES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv('text.csv', header=None, names=[\"text\", \"label\"])\n",
        "texts = data['text']  # Extract text column\n",
        "labels = data['label'].map({'pos':1,'neg':0})  # Extract label column\n",
        "\n",
        "# Convert text to feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Input a single text and predict its label\n",
        "def predict_label(text):\n",
        "    text_vector = vectorizer.transform([text])  # Convert input text to feature vector\n",
        "    label = model.predict(text_vector)[0]  # Predict label (0 or 1)\n",
        "    return \"Positive\" if label == 1 else \"Negative\"\n",
        "\n",
        "# Test with an input\n",
        "input_text = \"I enjoy coding\"\n",
        "predicted_label = predict_label(input_text)\n",
        "print(f\"Input: '{input_text}'\")\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "\n",
        "\n",
        "\n",
        "#NAIVE BAYES\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample text data\n",
        "texts = [\"I love programming\", \"Python is great\", \"I dislike bugs\", \"Debugging is fun\", \"I love solving problems\"]\n",
        "labels = [1, 1, 0, 1, 1]  # 1: Positive, 0: Negative\n",
        "\n",
        "# Convert text to feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Input a single text and predict its label\n",
        "def predict_label(text):\n",
        "    text_vector = vectorizer.transform([text])  # Convert input text to feature vector\n",
        "    label = model.predict(text_vector)[0]  # Predict label (0 or 1)\n",
        "    return \"Positive\" if label == 1 else \"Negative\"\n",
        "\n",
        "# Test with an input\n",
        "input_text = \"I enjoy coding\"\n",
        "predicted_label = predict_label(input_text)\n",
        "print(f\"Input: '{input_text}'\")\n",
        "print(f\"Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "WXnuwLRysaJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "NGqaZK5bsdko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data[:, :2]  # Use only the first two features (sepal length and sepal width)\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Visualization\n",
        "# Create a mesh grid to plot the decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Predict for every point in the mesh grid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', label='Training data', cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Plot the testing points\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', marker='x', label='Test data', cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Labels and legend\n",
        "plt.title('KNN Classifier - Decision Boundaries (Iris Dataset)')\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Sepal Width')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u03pWYj6sgvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "Rwl6bWc2skAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the important packages\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "cancer = load_iris()\n",
        "X = cancer.data[:, :2]\n",
        "y = cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Build the model\n",
        "svm = SVC(kernel=\"linear\")\n",
        "# Trained the model\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# # Plot Decision Boundary\n",
        "# DecisionBoundaryDisplay.from_estimator(\n",
        "#         svm,\n",
        "#         X,\n",
        "#         response_method=\"predict\",\n",
        "\n",
        "#         alpha=0.8,\n",
        "#         xlabel=cancer.feature_names[0],\n",
        "#         ylabel=cancer.feature_names[1],\n",
        "#     )\n",
        "\n",
        "# Scatter plot\n",
        "# plt.scatter(X[:, 0], X[:, 1],\n",
        "#             c=y,\n",
        "#             s=100, edgecolors=\"k\")\n",
        "# plt.show()\n",
        "\n",
        "# Visualization\n",
        "# Create a mesh grid to plot the decision boundaries\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Predict for every point in the mesh grid\n",
        "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', label='Training data', cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Plot the testing points\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, edgecolors='k', marker='x', label='Test data', cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Labels and legend\n",
        "plt.title('KNN Classifier - Decision Boundaries (Iris Dataset)')\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Sepal Width')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0XZSa4-oslHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM"
      ],
      "metadata": {
        "id": "V4ICn1H_spPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary libraries\n",
        "!pip install hmmlearn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Define the state space\n",
        "states = [\"Sunny\", \"Rainy\"]\n",
        "n_states = len(states)\n",
        "print('Number of hidden states :',n_states)\n",
        "# Define the observation space\n",
        "observations = [\"Dry\", \"Wet\"]\n",
        "n_observations = len(observations)\n",
        "print('Number of observations :',n_observations)\n",
        "\n",
        "\n",
        "# Define the initial state distribution\n",
        "state_probability = np.array([0.6, 0.4])\n",
        "print(\"State probability: \", state_probability)\n",
        "\n",
        "# Define the state transition probabilities\n",
        "transition_probability = np.array([[0.7, 0.3],\n",
        "\t\t\t\t\t\t\t\t[0.3, 0.7]])\n",
        "print(\"\\nTransition probability:\\n\", transition_probability)\n",
        "# Define the observation likelihoods\n",
        "emission_probability= np.array([[0.9, 0.1],\n",
        "\t\t\t\t\t\t\t\t[0.2, 0.8]])\n",
        "print(\"\\nEmission probability:\\n\", emission_probability)\n",
        "\n",
        "\n",
        "model = hmm.CategoricalHMM(n_components=n_states)\n",
        "model.startprob_ = state_probability\n",
        "model.transmat_ = transition_probability\n",
        "model.emissionprob_ = emission_probability\n",
        "\n",
        "\n",
        "# Define the sequence of observations\n",
        "observations_sequence = np.array([0, 1, 0, 1, 0, 0]).reshape(-1, 1)\n",
        "observations_sequence\n",
        "\n",
        "\n",
        "# Predict the most likely sequence of hidden states\n",
        "hidden_states = model.predict(observations_sequence)\n",
        "print(\"Most likely hidden states:\", hidden_states)\n",
        "\n",
        "\n",
        "log_probability, hidden_states = model.decode(observations_sequence,\n",
        "\t\t\t\t\t\t\t\t\t\t\tlengths = len(observations_sequence),\n",
        "\t\t\t\t\t\t\t\t\t\t\talgorithm ='viterbi')\n",
        "\n",
        "print('Log Probability :',log_probability)\n",
        "print(\"Most likely hidden states:\", hidden_states)\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.plot(hidden_states, '-o', label=\"Hidden State\")\n",
        "plt.xlabel('Time step')\n",
        "plt.ylabel('Most Likely Hidden State')\n",
        "plt.title(\"Sunny or Rainy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_x6Maj7asqz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FAce"
      ],
      "metadata": {
        "id": "fovwSYg6subv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "x=cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
        "image=cv2.imread(\"soft copy.jpg\")\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "faces = x.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "if len(faces) > 0:\n",
        "        print(f\"Faces detected: {len(faces)}\")\n",
        "else:\n",
        "        print(\"No face detected.\")\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "JaMWOU30svzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hierarchy"
      ],
      "metadata": {
        "id": "qnmlZFEts0kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Feature data\n",
        "y = iris.target  # True labels (optional, for reference)\n",
        "\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method='average'))\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Euclidean Distance\")\n",
        "plt.show()\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le=LabelEncoder()\n",
        "# Load Iris dataset\n",
        "\n",
        "iris =pd.read_csv('Customers.csv')\n",
        "X = iris[0:-1] # Feature data\n",
        "# X=le.fit_transform(X)\n",
        " # True labels (optional, for reference)\n",
        "X = pd.get_dummies(X)\n",
        "\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method='average'))\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Euclidean Distance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NuSdvVYOs2mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kmeans"
      ],
      "metadata": {
        "id": "m6xyEAGqs7TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Step 1: Generate synthetic data with make_blobs\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Step 2: Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)  # Set number of clusters\n",
        "kmeans.fit(X)  # Fit the KMeans model to the data\n",
        "\n",
        "# Step 3: Get the centroids and labels from the KMeans model\n",
        "centroids = kmeans.cluster_centers_  # Centroids of the clusters\n",
        "labels = kmeans.labels_  # Labels assigned to each data point\n",
        "\n",
        "\n",
        "# Step 4: Visualize the clustering results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, marker='o', s=100, alpha=0.7)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red', s=200, label='Centroids')\n",
        "plt.title('K-Means Clustering with make_blobs Dataset')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WeZ2Gg4Hs_L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN"
      ],
      "metadata": {
        "id": "q2r7-uvttAH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.9, min_samples=5)\n",
        "y_dbscan = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_dbscan,marker='o')\n",
        "plt.title(\"DBSCAN Clustering with make_blobs Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jDvp26lRtBVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}